{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 루브릭\n",
    "1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?  \n",
    "   챗봇 훈련데이터를 위한 전처리와 Augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.  \n",
    "2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?  \n",
    "   과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.  \n",
    "3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?  \n",
    "   주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 12:25:08.035909: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 12:25:08.893242: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드\n",
    "ChabotData.csv를 pandas를 이용해 읽어온 후, 데이터의 질문과 답변을 각각 question, answers 변수에 나눠서 저장.  \n",
    "[songys/Chatbot_data](https://github.com/songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/ChatbotData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df['Q']\n",
    "answers = df['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제\n",
    "preprocess_sentence() 함수 구현  \n",
    "- 영문자의 경우, 모두 소문자로 변환\n",
    "- 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거  \n",
    "\n",
    "이전과 다르게 생략된 기능들은 토크나이저에서 지원."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"[^0-9ㄱ-ㅣ가-힣a-zA-Z?.!]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 토큰화\n",
    "KoNLPy의 mecab 클래스 사용.  \n",
    "build_corpus() 함수 구현\n",
    "- 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받음.\n",
    "- 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화\n",
    "- 토큰화는 전달받은 토크나이즈 함수를 사용. mecab.morphs 함수를 전달\n",
    "- 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외\n",
    "- 중복되는 문장은 데이터에서 제외. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사. 중복 쌍이 흐트러지지 않도록 유의.  \n",
    "\n",
    "구현한 함수를 활용하여 qeustions와 answers를 각각 que_corpus, ans_corpus에 토큰화하여 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src_sentences, tgt_sentences, tokenize_fn, max_len = 40):\n",
    "    src_corpus = []\n",
    "    tgt_corpus = []\n",
    "    src_seen = set()\n",
    "    tgt_seen = set()\n",
    "\n",
    "    for src, tgt in zip(src_sentences, tgt_sentences):\n",
    "        src = preprocess_sentence(src)\n",
    "        tgt = preprocess_sentence(tgt)\n",
    "\n",
    "        src_tokens = tokenize_fn(src)\n",
    "        tgt_tokens = tokenize_fn(tgt)\n",
    "\n",
    "        if len(src_tokens) <= max_len and len(tgt_tokens) <= max_len:\n",
    "            src_joined = ' '.join(src_tokens)\n",
    "            tgt_joined = ' '.join(tgt_tokens)\n",
    "            if src_joined not in src_seen and tgt_joined not in tgt_seen:\n",
    "                src_corpus.append(src_tokens)\n",
    "                tgt_corpus.append(tgt_tokens)\n",
    "                src_seen.add(src_joined)\n",
    "                tgt_seen.add(tgt_joined)\n",
    "\n",
    "    return src_corpus, tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7683 7683\n",
      "\n",
      "que_corpus: \n",
      "12 시 땡 !\n",
      "1 지망 학교 떨어졌 어\n",
      "3 박 4 일 놀 러 가 고 싶 다\n",
      "ppl 심하 네\n",
      "sd 카드 망가졌 어\n",
      "\n",
      "ans_corpus: \n",
      "하루 가 또 가 네요 .\n",
      "위로 해 드립니다 .\n",
      "여행 은 언제나 좋 죠 .\n",
      "눈살 이 찌푸려 지 죠 .\n",
      "다시 새로 사 는 게 마음 편해요 .\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab()\n",
    "\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers, mecab.morphs)\n",
    "\n",
    "print(len(que_corpus), len(ans_corpus))\n",
    "print()\n",
    "print('que_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in que_corpus[:5]]), sep = '\\n')\n",
    "print()\n",
    "print('ans_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in ans_corpus[:5]]), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Augmentation\n",
    "Lexical Substitution을 실제로 적용.  \n",
    "아래 링크를 참고하여 한국어로 사전 훈련된 Embedding 모델을 다운로드. Korean (w)가 Word2Vec으로 학습한 모델이며 용량도 적당. Korean (w) 다운로드. ko.bin 파일 얻기.  \n",
    "[Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)  \n",
    "다운로드한 모델을 활용해 데이터 Augmentation. lexical_sub() 함수 참고.  \n",
    "Augmentation된 que_corpus와 원본 ans_corpus가 병렬을 이루도록, 원본 que_corpus와 Augmentation된 ans_corpus가 병렬을 이루도록 하여 전체 데이터가 원래의 3배 가량으로 늘어나도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv = gensim.models.Word2Vec.load('./data/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv = gensim.models.KeyedVectors.load_word2vec_format('./data/ko.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = Word2VecKeyedVectors.load('./data/word2vec_ko.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(tokens):\n",
    "    selected_tok = random.choice(tokens)\n",
    "\n",
    "    try:\n",
    "        similar_word = word_vectors.wv.similar_by_word(selected_tok)[0][0]\n",
    "    except KeyError:\n",
    "        similar_word = selected_tok\n",
    "        # print('not changed', 'tokens:', tokens)\n",
    "    \n",
    "    return [similar_word if tok == selected_tok else tok for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_data(que_corpus, ans_corpus):\n",
    "    augmented_que_corpus = []\n",
    "    augmented_ans_corpus = []\n",
    "\n",
    "    augmented_que_corpus.extend(que_corpus)\n",
    "    augmented_ans_corpus.extend(ans_corpus)\n",
    "\n",
    "    for question in que_corpus:\n",
    "        augmented_question = lexical_sub(question)\n",
    "        augmented_que_corpus.append(augmented_question)\n",
    "        augmented_ans_corpus.append(ans_corpus[que_corpus.index(question)])\n",
    "    \n",
    "    for answer in ans_corpus:\n",
    "        augmented_answer = lexical_sub(answer)\n",
    "        augmented_ans_corpus.append(augmented_answer)\n",
    "        augmented_que_corpus.append(que_corpus[ans_corpus.index(answer)])\n",
    "    \n",
    "    return augmented_que_corpus, augmented_ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23049 23049\n",
      "\n",
      "que_corpus: \n",
      "12 시 땡 !\n",
      "1 지망 학교 떨어졌 어\n",
      "3 박 4 일 놀 러 가 고 싶 다\n",
      "ppl 심하 네\n",
      "sd 카드 망가졌 어\n",
      "\n",
      "ans_corpus: \n",
      "하루 가 또 가 네요 .\n",
      "위로 해 드립니다 .\n",
      "여행 은 언제나 좋 죠 .\n",
      "눈살 이 찌푸려 지 죠 .\n",
      "다시 새로 사 는 게 마음 편해요 .\n"
     ]
    }
   ],
   "source": [
    "que_corpus, ans_corpus = augmentation_data(que_corpus, ans_corpus)\n",
    "\n",
    "print(len(que_corpus), len(ans_corpus))\n",
    "print()\n",
    "print('que_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in que_corpus[:5]]), sep = '\\n')\n",
    "print()\n",
    "print('ans_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in ans_corpus[:5]]), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_duplicate(que_corpus, ans_corpus):\n",
    "    que_tokens = []\n",
    "    ans_tokens = []\n",
    "\n",
    "    que_seen = set()\n",
    "    ans_seen = set()\n",
    "\n",
    "    for i in range(len(que_corpus)):\n",
    "        que_joined = ' '.join(que_corpus[i])\n",
    "        ans_joined = ' '.join(ans_corpus[i])\n",
    "\n",
    "        if que_joined in que_seen and ans_joined in ans_seen:\n",
    "            pass\n",
    "        else:\n",
    "            que_tokens.append(que_corpus[i])\n",
    "            ans_tokens.append(ans_corpus[i])\n",
    "            que_seen.add(que_joined)\n",
    "            ans_seen.add(ans_joined)\n",
    "    \n",
    "    return que_tokens, ans_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22840 22840\n",
      "\n",
      "que_corpus: \n",
      "12 시 땡 !\n",
      "1 지망 학교 떨어졌 어\n",
      "3 박 4 일 놀 러 가 고 싶 다\n",
      "ppl 심하 네\n",
      "sd 카드 망가졌 어\n",
      "\n",
      "ans_corpus: \n",
      "하루 가 또 가 네요 .\n",
      "위로 해 드립니다 .\n",
      "여행 은 언제나 좋 죠 .\n",
      "눈살 이 찌푸려 지 죠 .\n",
      "다시 새로 사 는 게 마음 편해요 .\n"
     ]
    }
   ],
   "source": [
    "que_corpus, ans_corpus = delete_duplicate(que_corpus, ans_corpus)\n",
    "\n",
    "print(len(que_corpus), len(ans_corpus))\n",
    "print()\n",
    "print('que_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in que_corpus[:5]]), sep = '\\n')\n",
    "print()\n",
    "print('ans_corpus: ',\n",
    "      '\\n'.join([' '.join(tokens) for tokens in ans_corpus[:5]]), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. 데이터 벡터화\n",
    "ans_corpus에 \\<start\\> 토큰과 \\<end\\> 토큰을 추가 후 벡터화 진행.  \n",
    "```python\n",
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])\n",
    "```\n",
    "챗봇 훈련 데이터는 소스 데이터와 타겟 데이터가 같은 언어를 사용. Embedding 층을 공유했을 때 많은 이점.  \n",
    "- ans_corpus와 que_corpus를 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 enc_train과 dec_train을 얻기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "\n",
    "ans_corpus = [[start_token] + ans + [end_token] for ans in ans_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7593\n",
      "Encoder Training Shape: (22840, 41)\n",
      "Decoder Training Shape: (22840, 41)\n",
      "Sample Encoder Sequence: [2215  215 3416  111    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "Sample Decoder Sequence: [  3 279   9 149   9  43   2   4   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "total_corpus = que_corpus + ans_corpus\n",
    "\n",
    "tokenizer = Tokenizer(filters = '', oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(total_corpus)\n",
    "\n",
    "start_id = tokenizer.word_index[start_token]\n",
    "end_id = tokenizer.word_index[end_token]\n",
    "\n",
    "que_sequences = tokenizer.texts_to_sequences(que_corpus)\n",
    "ans_sequences = tokenizer.texts_to_sequences(ans_corpus)\n",
    "\n",
    "max_len = max(max(len(seq) for seq in que_sequences), max(len(seq) for seq in ans_sequences))\n",
    "enc_train = pad_sequences(que_sequences, padding = 'post', maxlen = max_len)\n",
    "dec_train = pad_sequences(ans_sequences, padding = 'post', maxlen = max_len)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
    "print(f\"Encoder Training Shape: {enc_train.shape}\")\n",
    "print(f\"Decoder Training Shape: {dec_train.shape}\")\n",
    "\n",
    "print(\"Sample Encoder Sequence:\", enc_train[0])\n",
    "print(\"Sample Decoder Sequence:\", dec_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. 훈련하기\n",
    "앞서 번역 모델을 훈련하며 정의한 Transformer를 그대로 사용.  \n",
    "데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있음.  \n",
    "모델을 훈련하고 아래 예문에 대한 답변을 생성.\n",
    "```python\n",
    "# 예문\n",
    "1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "```\n",
    "---\n",
    "```python\n",
    "# 제출\n",
    "\n",
    "Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 12:28:13.037812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20628 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=1,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=1024,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "d_model = 512\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/357 [00:00<?, ?it/s]2024-07-16 12:28:18.294325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-07-16 12:28:18.369984: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7a739a0dc0a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-16 12:28:18.370032: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2024-07-16 12:28:18.380797: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-16 12:28:18.407935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-07-16 12:28:18.592161: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "100%|██████████| 357/357 [00:51<00:00,  6.89it/s, loss=5.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 5.841907661502101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 357/357 [00:08<00:00, 44.15it/s, loss=3.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 3.724851122089461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 357/357 [00:08<00:00, 44.03it/s, loss=2.83]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 2.8295937759869574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Q. 위의 코드를 활용하여 모델을 훈련시켜봅시다!\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    \n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # tqdm 업데이트\n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_postfix(loss=total_loss.numpy() / (batch + 1))\n",
    "    \n",
    "    tqdm_bar.close()\n",
    "    print(f'Epoch {epoch + 1} Loss {total_loss.numpy() / dataset_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. 성능 측정하기\n",
    "주어진 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 calculate_bleu() 함수 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, tokenizer, max_len = max_len):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    pieces = mecab.morphs(sentence)\n",
    "    tokens = tokenizer.texts_to_sequences([' '.join(pieces)])\n",
    "\n",
    "    _input = pad_sequences(tokens, maxlen=max_len, padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([start_id], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if end_id == predicted_id:\n",
    "            result = tokenizer.sequences_to_texts([ids])\n",
    "            return pieces, ' '.join(result), enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tokenizer.sequences_to_texts(ids)\n",
    "\n",
    "    return pieces, ' '.join(result), enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 사랑 은 사랑 은 사랑 이 필요 해요 .\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 마음 은 잘 되 었 나 봐요 .\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 좋 은 선택 이 었 나 봐요 .\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 좋 아 하 는 것 이 에요 .\n"
     ]
    }
   ],
   "source": [
    "examples = ['지루하다, 놀러가고 싶어.',\n",
    "            '오늘 일찍 일어났더니 피곤하다.',\n",
    "            '간만에 여자친구랑 데이트 하기로 했어.',\n",
    "            '집에 있는다는 소리야.']\n",
    "\n",
    "candidates = []\n",
    "for example in examples:\n",
    "    candidates.append(translate(example, transformer, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3181818181818182\n",
      "BLEU-2: 0.09523809523809525\n",
      "BLEU-3: 0.05000000000000001\n",
      "BLEU-4: 0.005263157894736842\n",
      "BLEU-Total: 0.05314049749131566\n",
      "\n",
      "\n",
      "BLEU-1: 0.35294117647058826\n",
      "BLEU-2: 0.12500000000000003\n",
      "BLEU-3: 0.06666666666666667\n",
      "BLEU-4: 0.007142857142857146\n",
      "BLEU-Total: 0.06770149544242768\n",
      "\n",
      "\n",
      "BLEU-1: 0.29411764705882354\n",
      "BLEU-2: 0.0625\n",
      "BLEU-3: 0.006666666666666668\n",
      "BLEU-4: 0.007142857142857146\n",
      "BLEU-Total: 0.03058760346458022\n",
      "\n",
      "\n",
      "BLEU-1: 0.42733711854819223\n",
      "BLEU-2: 0.26589865154109743\n",
      "BLEU-3: 0.20349386597532967\n",
      "BLEU-4: 0.13148834416867455\n",
      "BLEU-Total: 0.23481797190640163\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "references = ['잠깐 쉬 어도 돼요 . <end>',\n",
    "              '맛난 거 드세요 . <end>',\n",
    "              '떨리 겠 죠 . <end>',\n",
    "              '좋 아 하 면 그럴 수 있 어요 . <end>']\n",
    "\n",
    "for reference, candidate in zip(references, candidates):\n",
    "    print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "    print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "    print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "    print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "    print(\"BLEU-Total:\", calculate_bleu(reference, candidate))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, epochs = 3):\n",
    "    learning_rate = LearningRateScheduler(params['d_model'], warmup_steps = 1000)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)\n",
    "\n",
    "    transformer = Transformer(\n",
    "        n_layers = int(params['n_layers']),\n",
    "        d_model = int(params['d_model']),\n",
    "        n_heads = int(params['n_heads']),\n",
    "        d_ff = int(params['d_ff']),\n",
    "        src_vocab_size = VOCAB_SIZE,\n",
    "        tgt_vocab_size = VOCAB_SIZE,\n",
    "        pos_len = 200,\n",
    "        dropout = params['dropout'],\n",
    "        shared_fc = True,\n",
    "        shared_emb = True\n",
    "    )\n",
    "\n",
    "    # Train Step 정의\n",
    "    @tf.function()\n",
    "    def train_step(src, tgt, model, optimizer):\n",
    "        tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "        gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "            loss = loss_function(gold, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch, (src, tgt) in enumerate(train_dataset):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "        print(f'Epoch {epoch + 1} Loss {total_loss.numpy() / dataset_count}')\n",
    "\n",
    "    return total_loss.numpy() / dataset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 4.546649665725665                       \n",
      "Epoch 2 Loss 2.4272226712950804                      \n",
      "Epoch 3 Loss 1.5802085726868873                      \n",
      "Epoch 1 Loss 4.713446267178747                                                 \n",
      "Epoch 2 Loss 2.9203616016719187                                                \n",
      "Epoch 3 Loss 2.126684878052784                                                 \n",
      "Epoch 1 Loss 5.03676983488708                                                  \n",
      "Epoch 2 Loss 3.1375975194765404                                                \n",
      "Epoch 3 Loss 2.387592048538165                                                 \n",
      "Epoch 1 Loss 4.8727582830007                                                   \n",
      "Epoch 2 Loss 3.1527489413734244                                                \n",
      "Epoch 3 Loss 2.4724651090905114                                                \n",
      "Epoch 1 Loss 4.512456984747024                                                 \n",
      "Epoch 2 Loss 2.5064167268469886                                                \n",
      "Epoch 3 Loss 1.9830517167804622                                                \n",
      "100%|██████████| 5/5 [06:34<00:00, 78.82s/trial, best loss: 1.5802085726868873]\n",
      "{'d_ff': 0, 'd_model': 2, 'dropout': 0.4357252943554374, 'n_heads': 1, 'n_layers': 0}\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'n_layers': hp.choice('n_layers', [1, 2, 3]),\n",
    "    'd_model': hp.choice('d_model', [128, 256, 512]),\n",
    "    'n_heads': hp.choice('n_heads', [4, 8]),\n",
    "    'd_ff': hp.choice('d_ff', [512, 1024, 2048]),\n",
    "    'dropout': hp.uniform('dropout', 0.1, 0.5)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "model = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.5802085726868873, 'status': 'ok'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial = trials.best_trial\n",
    "best_trial['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. 성능 측정하기\n",
    "주어진 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 calculate_bleu() 함수 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, tokenizer, max_len = max_len):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    pieces = mecab.morphs(sentence)\n",
    "    tokens = tokenizer.texts_to_sequences([' '.join(pieces)])\n",
    "\n",
    "    _input = pad_sequences(tokens, maxlen=max_len, padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([start_id], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if end_id == predicted_id:\n",
    "            result = tokenizer.sequences_to_texts([ids])\n",
    "            return pieces, ' '.join(result), enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tokenizer.sequences_to_texts(ids)\n",
    "\n",
    "    return pieces, ' '.join(result), enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 사랑 은 사랑 은 사랑 이 필요 해요 .\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 마음 은 잘 되 었 나 봐요 .\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 좋 은 선택 이 었 나 봐요 .\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 좋 아 하 는 것 이 에요 .\n"
     ]
    }
   ],
   "source": [
    "examples = ['지루하다, 놀러가고 싶어.',\n",
    "            '오늘 일찍 일어났더니 피곤하다.',\n",
    "            '간만에 여자친구랑 데이트 하기로 했어.',\n",
    "            '집에 있는다는 소리야.']\n",
    "\n",
    "candidates = []\n",
    "for example in examples:\n",
    "    candidates.append(translate(example, transformer, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.3181818181818182\n",
      "BLEU-2: 0.09523809523809525\n",
      "BLEU-3: 0.05000000000000001\n",
      "BLEU-4: 0.005263157894736842\n",
      "BLEU-Total: 0.05314049749131566\n",
      "\n",
      "\n",
      "BLEU-1: 0.35294117647058826\n",
      "BLEU-2: 0.12500000000000003\n",
      "BLEU-3: 0.06666666666666667\n",
      "BLEU-4: 0.007142857142857146\n",
      "BLEU-Total: 0.06770149544242768\n",
      "\n",
      "\n",
      "BLEU-1: 0.29411764705882354\n",
      "BLEU-2: 0.0625\n",
      "BLEU-3: 0.006666666666666668\n",
      "BLEU-4: 0.007142857142857146\n",
      "BLEU-Total: 0.03058760346458022\n",
      "\n",
      "\n",
      "BLEU-1: 0.42733711854819223\n",
      "BLEU-2: 0.26589865154109743\n",
      "BLEU-3: 0.20349386597532967\n",
      "BLEU-4: 0.13148834416867455\n",
      "BLEU-Total: 0.23481797190640163\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "references = ['잠깐 쉬 어도 돼요 . <end>',\n",
    "              '맛난 거 드세요 . <end>',\n",
    "              '떨리 겠 죠 . <end>',\n",
    "              '좋 아 하 면 그럴 수 있 어요 . <end>']\n",
    "\n",
    "for reference, candidate in zip(references, candidates):\n",
    "    print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "    print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "    print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "    print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "    print(\"BLEU-Total:\", calculate_bleu(reference, candidate))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: 지루하다, 놀러가고 싶어.  \n",
    "Predicted translation: 사랑 은 사랑 은 사랑 이 필요 해요 .  \n",
    "  \n",
    "Input: 오늘 일찍 일어났더니 피곤하다.  \n",
    "Predicted translation: 마음 은 잘 되 었 나 봐요 .  \n",
    "  \n",
    "Input: 간만에 여자친구랑 데이트 하기로 했어.  \n",
    "Predicted translation: 좋 은 선택 이 었 나 봐요 .  \n",
    "  \n",
    "Input: 집에 있는다는 소리야.  \n",
    "Predicted translation: 좋 아 하 는 것 이 에요 .  \n",
    "  \n",
    "n_layers: 1  \n",
    "d_model: 512  \n",
    "n_heads: 8  \n",
    "d_ff: 1024  \n",
    "dropout: 0.3  \n",
    "  \n",
    "Warmup Steps: 4000\n",
    "Batch Size: 64  \n",
    "Epoch At: 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
