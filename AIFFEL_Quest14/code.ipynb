{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21-1. 프로젝트 : mini BERT 만들기  \n",
    "vocab size를 8000으로 줄이고, 전체 파라미터 사이즈가 1M 정도가 되는 아주 작은 mini BERT 모델을 만들어 10 Epoch까지 학습시킨 모델을 만들어 보는 것입니다.  \n",
    "## 루브릭\n",
    "1. 한글 코퍼스를 가공하여 BERT pretrain용 데이터셋을 잘 생성하였다.  \n",
    "   MLM, NSP task의 특징이 잘 반영된 pretrain용 데이터셋 생성과정이 체계적으로 진행되었다.\n",
    "2. 구현한 BERT 모델의 학습이 안정적으로 진행됨을 확인하였다.  \n",
    "   학습진행 과정 중에 MLM, NSP loss의 안정적인 감소가 확인되었다.\n",
    "3. 1M짜리 mini BERT 모델의 제작과 학습이 정상적으로 진행되었다.  \n",
    "   학습된 모델 및 학습과정의 시각화 내역이 제출되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer 준비\n",
    "SentencePiece 모델을 이용해 BERT의 MLM 학습용 데이터를 만드세요.  \n",
    "  \n",
    "이를 위해 한글 나무 위키 코퍼스로부터 8000의 vocab_size를 갖는 sentencepiece 모델을 만들어 보세요. BERT에 사용되는 주요 특수문자가 vocab에 포함되어야 합니다. (시간이 부족하다면 클라우드에 저장된 sentencepiece 모델을 사용하세요.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 (1) MASK 생성\n",
    "BERT의 MLM에 필요한 빈칸(mask)을 학습 데이터 전체 토큰의 15% 정도로 만들어 주세요. 그 중 80%는 [MASK] 토큰, 10%는 랜덤한 토큰, 나머지 10%는 원래의 토큰을 그대로 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    # tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "    tokens = copy.deepcopy(tokens)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    \n",
    "    # 순서 정렬 및 mask_idx, mask_label 생성\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리 (2) NSP pair 생성\n",
    "BERT의 pretrain task인 NSP는 두 문장이 연속하는지 확인하는 것입니다. 이를 위해 2개의 문장을 짝지어 50%의 확률로 TRUE와 FALSE를 지정해 주세요.  \n",
    "  \n",
    "두 문장 사이에 segment 처리를 해주세요. 첫 번째 문장의 segment는 0, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주세요.  \n",
    "  \n",
    "MLM과 NSP는 동시에 학습된다는 것을 염두에 두고 학습 데이터를 구성해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < mask_prob:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            \n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "    \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 전처리 (3) 데이터셋 완성\n",
    "BERT pretrain 데이터셋을 생성해, json 포맷으로 저장하세요. 데이터셋의 사이즈가 크므로np.memmap을 사용해 메모리 사용량을 최소화해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if len(doc) > 0:\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if len(pieces) > 0:\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2c3edb53864d3a92d999f573fe01ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "# make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630496"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int32)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int32)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int32, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce91923b32e426aba5ce4bc99000011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   10, 1605, 3599, 1755, 3630,   41, 3644,  830, 3624, 1135,\n",
       "           52, 3599,   13,   81,   87, 1501, 2247,   25, 3779, 3873, 3667,\n",
       "         3631, 3813, 3873, 4196, 3636, 3779, 3601,  249, 3725, 1232,   33,\n",
       "           52, 3599,    6,    6,    6, 6322, 2780,   14, 1509,  168, 3877,\n",
       "          414,  165, 1697, 4290, 3873, 3703, 3683,  593,   21, 5007,  399,\n",
       "         1927, 3607,    6,    6,    6,    6,    6,    6,  103, 4313, 4290,\n",
       "          613, 3638, 3718,   98, 3878, 3656,  256, 2543,  309,  337, 3735,\n",
       "          181, 3616, 3603,  489,  376, 3599,    4,    6,    6,  207, 3714,\n",
       "            6, 1042,  103, 3610, 3686, 3718,    6,    6,   37, 3418,  416,\n",
       "          810, 3666, 3625,  131, 3662,    7, 3629,  203,  241, 3602, 1114,\n",
       "         3724,  788,  243,    6,    6,    6,  663, 1647, 3682, 3682, 3625,\n",
       "          203, 3008, 3625, 3616,   16, 3599,    4], dtype=int32),\n",
       " memmap([   5,  192, 3695, 1380,  664, 1076,  761, 2771, 1180, 3721, 3778,\n",
       "          263, 3600, 3920, 3597,    6,    6,    6,    6,    6,   13,  129,\n",
       "         3780, 3920, 3601,  546, 4166, 3597,   35, 3813, 3053,   54,  702,\n",
       "           69, 3599,   81, 3915, 3764,  923,  159,  294,  192, 3695, 1722,\n",
       "          433,  323,  217, 3205, 2222,  354,  366, 3697,  815, 1174,    6,\n",
       "         3024,  267,  105, 3624,    6,    6,    6,  171, 3599,    4,    6,\n",
       "            6,    6, 2015, 3607, 3438, 3633, 4210, 3781,    6,    6, 1862,\n",
       "         1722, 3082, 3612, 2015, 3597,  807, 3657, 3641, 3703, 3617, 3785,\n",
       "         4049,  343,   26, 3819, 3101, 3607,  374,  170,  812, 3688, 3720,\n",
       "         3683, 3603,  751,    6,    6, 1156, 4301, 3814, 3597,  708, 3968,\n",
       "           19, 1333, 2088,  546, 4166, 3601,  560,  879,  409, 3630,  979,\n",
       "         3599,    6,    6,  450, 3703,  456,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 0,\n",
       " 1,\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  479, 3652, 3625,  243,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  813,   17, 3599,  307,  587,  931,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,   18, 3686,    0,    0,\n",
       "         3324,    0,    0,    0,    0,    0,  207, 3714,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,   49, 3632,  796,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,  129, 3780, 3920,  211, 3604,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  355,\n",
       "            0,    0,    0,    0,  186, 3827, 1816,    0,    0,    0,   56,\n",
       "         3745, 1048,    0,    0,    0,    0,    0,    0,  456, 3963,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,  812, 3688,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,   13, 1659,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BERT 모델 구현\n",
    "pad mask, ahead mask 함수, gelu activation 함수, parameter initializer 생성 함수, json을 config 형태로 사용하기 위한 유틸리티 함수를 먼저 만들어 두세요.  \n",
    "  \n",
    "Embedding 레이어, Transformer encoder 레이어, BERT 레이어를 구성한 후, pretraine용 BERT 모델을 만들어 봅시다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        context = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        context = tf.transpose(context, [0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        context = tf.reshape(context, [batch_size, -1, self.n_head * self.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(context)  # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 128,\n",
       " 'n_head': 2,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 256,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 2,\n",
       " 'n_seq': 128,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"d_model\": 128,\n",
    "    \"n_head\": 2,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"d_ff\": 256,\n",
    "    \"layernorm_epsilon\": 0.001,\n",
    "    \"n_layer\": 2,\n",
    "    \"n_seq\": 128,\n",
    "    \"n_vocab\": len(vocab),\n",
    "    \"i_pad\": vocab.pad_id()\n",
    "})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. pretrain 진행\n",
    "loss, accuracy 함수를 정의하고 Learning Rate 스케쥴링을 구현한 후, 10 Epoch까지 모델 학습을 시켜보세요. 학습을 진행할 때는 배치 사이즈에 유의하세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 128), (None, 1306752     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            16768       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 1,323,520\n",
      "Trainable params: 1,323,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 118s 57ms/step - loss: 21.0647 - nsp_loss: 0.4849 - mlm_loss: 20.5798 - nsp_acc: 0.8501 - mlm_lm_acc: 0.0594\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.05943, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 18.6141 - nsp_loss: 0.4632 - mlm_loss: 18.1509 - nsp_acc: 0.8501 - mlm_lm_acc: 0.1154\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.05943 to 0.11541, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 17.8910 - nsp_loss: 0.4632 - mlm_loss: 17.4278 - nsp_acc: 0.8501 - mlm_lm_acc: 0.1288\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.11541 to 0.12885, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 17.4798 - nsp_loss: 0.4632 - mlm_loss: 17.0166 - nsp_acc: 0.8501 - mlm_lm_acc: 0.1355\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.12885 to 0.13549, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 17.0345 - nsp_loss: 0.4632 - mlm_loss: 16.5713 - nsp_acc: 0.8501 - mlm_lm_acc: 0.1431\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.13549 to 0.14314, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 16.5912 - nsp_loss: 0.4632 - mlm_loss: 16.1281 - nsp_acc: 0.8501 - mlm_lm_acc: 0.1503\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.14314 to 0.15026, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 16.1929 - nsp_loss: 0.4632 - mlm_loss: 15.7297 - nsp_acc: 0.8501 - mlm_lm_acc: 0.1567\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.15026 to 0.15670, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 15.8474 - nsp_loss: 0.4345 - mlm_loss: 15.4129 - nsp_acc: 0.8779 - mlm_lm_acc: 0.1622\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.15670 to 0.16220, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 15.6632 - nsp_loss: 0.3988 - mlm_loss: 15.2644 - nsp_acc: 0.9133 - mlm_lm_acc: 0.1646\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.16220 to 0.16461, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 15.6089 - nsp_loss: 0.3963 - mlm_loss: 15.2126 - nsp_acc: 0.9158 - mlm_lm_acc: 0.1655\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.16461 to 0.16553, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(\n",
    "    pre_train_inputs,\n",
    "    pre_train_labels,\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    callbacks = [save_weights]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 프로젝트 결과\n",
    "학습된 모델과 학습과정을 시각화해 보세요. NSP와 MLM의 loss가 안정적으로 수렴하나요? 모델이 작기 때문에 loss가 잘 수렴하지 않을 수도 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
