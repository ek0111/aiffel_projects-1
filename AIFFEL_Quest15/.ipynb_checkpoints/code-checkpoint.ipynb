{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c01f8c",
   "metadata": {},
   "source": [
    "# 24-1. 프로젝트 : 커스텀 프로젝트 직접 만들기\n",
    "실습 코드에서 수행해 본 내용을 토대로, 이번에는 한국어 데이터셋에 도전해보겠습니다.\n",
    "\n",
    "앞서 본 GLUE benchmark의 한국어 버전 [KLUE benchmark](https://klue-benchmark.com/)를 들어보신 적 있나요?\n",
    "\n",
    "GLUE와 마찬가지로 한국어 자연어처리에 대한 이해도를 높이기 위해 만들어진 데이터셋 benchmark입니다. 총 8가지의 데이터셋이 있습니다. 다만 이번 시간에 진행할 프로젝트는 KLUE의 dataset을 활용하는 것이 아닌, model(klue/ber-base)를 활용하여 NSMC(Naver Sentiment Movie Corpus) task를 도전해보겠습니다.\n",
    "\n",
    "모델과 데이터에 관한 정보는 링크를 참조해주세요.\n",
    "- [KLUE/Bert-base](https://huggingface.co/klue/bert-base)\n",
    "- [NSMC](https://github.com/e9t/nsmc)\n",
    "  \n",
    "## 루브릭\n",
    "1. 모델과 데이터를 정상적으로 불러오고, 작동하는 것을 확인하였다.  \n",
    "   klue/bert-base를 NSMC 데이터셋으로 fine-tuning 하여, 모델이 정상적으로 작동하는 것을 확인하였다.  \n",
    "2. Preprocessing을 개선하고, fine-tuning을 통해 모델의 성능을 개선시켰다.  \n",
    "   Validation accuracy를 90% 이상으로 개선하였다.\n",
    "3. 모델 학습에 Bucketing을 성공적으로 적용하고, 그 결과를 비교분석하였다.  \n",
    "   Bucketing task을 수행하여 fine-tuning 시 연산 속도와 모델 성능 간의 trade-off 관계가 발생하는지 여부를 확인하고, 분석한 결과를 제시하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df858fa0",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b3bf3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f13812",
   "metadata": {},
   "source": [
    "## STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n",
    "데이터셋은 깃허브에서 다운받거나, [Huggingface datasets](https://huggingface.co/datasets)에서 가져올 수 있습니다. 앞에서 배운 방법들을 활용해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b67437a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Blpeng___nsmc-55757a98c8abea78\n",
      "Reusing dataset csv (/aiffel/.cache/huggingface/datasets/csv/Blpeng___nsmc-55757a98c8abea78/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb82fb15c8748bbb262f5a092264818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'id', 'document', 'label'],\n",
      "        num_rows: 400000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# NSMC 데이터셋 로드\n",
    "huggingface_nsmc_dataset = load_dataset('Blpeng/nsmc')\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(huggingface_nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3868ad13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0', 'id', 'document', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_nsmc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76611c92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0 : 0\n",
      "id : 8112052\n",
      "document : 어릴때보고 지금다시봐도 재밌어요ㅋㅋ\n",
      "type: True\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 1\n",
      "id : 8132799\n",
      "document : 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.\n",
      "type: True\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 2\n",
      "id : 4655635\n",
      "document : 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.\n",
      "type: True\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 3\n",
      "id : 9251303\n",
      "document : 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지\n",
      "type: True\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 4\n",
      "id : 10067386\n",
      "document : 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.\n",
      "type: True\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "        if col == 'document':\n",
    "            print('type:', isinstance(train[col][i], str))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d0e12b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# not_str_idecies = []\n",
    "# for sample in huggingface_nsmc_dataset['train']:\n",
    "#     if not isinstance(sample[document], str):\n",
    "#         not_str_idecies.append(sample['Unnamed: 0'])\n",
    "# #         print(sample[document])\n",
    "# print(not_str_idecies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134bbf2",
   "metadata": {},
   "source": [
    "None data Unnamed: 0: [46471, 60735, 77665, 84098, 127017, 172375, 173526, 197279, 5746, 7899, 27097, 25857, 55737, 110014, 126782, 140721]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d68e7c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f765f88dbf3541ffb715b4cea3d1c4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_none_examples(example):\n",
    "    return isinstance(example['document'], str) and example['document'].strip() != ''\n",
    "\n",
    "hf_dataset = huggingface_nsmc_dataset['train'].filter(filter_none_examples)\n",
    "hf_dataset = hf_dataset.shuffle(seed = 526).select(range(200000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d39f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'id', 'document', 'label'],\n",
      "    num_rows: 200000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b365d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "del huggingface_nsmc_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08fa7fe",
   "metadata": {},
   "source": [
    "## STEP 2. klue/bert-base model 및 tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b316371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'klue/bert-base'\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8e787",
   "metadata": {},
   "source": [
    "## STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f5daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        max_length = 20,\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6182fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d108eea8494245b2771b54f4cc3d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = hf_dataset.map(transform, batched = True)\n",
    "\n",
    "# 먼저 전체 데이터셋을 85%/15%로 나눕니다.\n",
    "train_test_split = hf_dataset.train_test_split(test_size=0.15)\n",
    "\n",
    "# 나눠진 85%의 train 데이터셋을 다시 90%/10%로 나눔.\n",
    "# validation 데이터셋이 전체의 10%가 되도록 하기 위해 비율을 0.1176으로 설정\n",
    "train_validation_split = train_test_split['train'].train_test_split(test_size=0.10 / 0.85)\n",
    "\n",
    "hf_train_dataset = train_validation_split['train']\n",
    "hf_val_dataset = train_validation_split['test']\n",
    "hf_test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe70ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "del hf_dataset, train_test_split, train_validation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "541af245",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    "    group_by_length = True,\n",
    "    gradient_accumulation_steps = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a5fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도와 F1 점수를 계산할 메트릭 로드\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # 정확도와 F1 점수 계산\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c08202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 전 메모리 비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e99ad1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id, Unnamed: 0.\n",
      "***** Running training *****\n",
      "  Num examples = 149999\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 3513\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3513' max='3513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3513/3513 43:04, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.269121</td>\n",
       "      <td>0.888106</td>\n",
       "      <td>0.887741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>0.245642</td>\n",
       "      <td>0.904105</td>\n",
       "      <td>0.903170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.249591</td>\n",
       "      <td>0.909005</td>\n",
       "      <td>0.908423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id, Unnamed: 0.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20001\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id, Unnamed: 0.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20001\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id, Unnamed: 0.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20001\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3513, training_loss=0.23322169311766675, metrics={'train_runtime': 2585.8272, 'train_samples_per_second': 174.024, 'train_steps_per_second': 1.359, 'total_flos': 4623827353581600.0, 'train_loss': 0.23322169311766675, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8ecc0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id, Unnamed: 0.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24641001224517822,\n",
       " 'eval_accuracy': 0.9084666666666666,\n",
       " 'eval_f1': 0.908801062769844,\n",
       " 'eval_runtime': 51.6636,\n",
       " 'eval_samples_per_second': 580.679,\n",
       " 'eval_steps_per_second': 72.585,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc384a6",
   "metadata": {},
   "source": [
    "## STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기\n",
    "데이터 전처리, TrainingArguments 등을 조정하여 모델의 정확도를 90% 이상으로 끌어올려봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568b776",
   "metadata": {},
   "source": [
    "{'eval_loss': 0.24641001224517822,  \n",
    " 'eval_accuracy': 0.9084666666666666,  \n",
    " 'eval_f1': 0.908801062769844,  \n",
    " 'eval_runtime': 51.6636,  \n",
    " 'eval_samples_per_second': 580.679,  \n",
    " 'eval_steps_per_second': 72.585,  \n",
    " 'epoch': 3.0}  \n",
    " 조건을 만족하여 모델 튜닝을 진행하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952cf100",
   "metadata": {},
   "source": [
    "## STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교\n",
    "아래 링크를 바탕으로 bucketing과 dynamic padding이 무엇인지 알아보고, 이들을 적용하여 model을 학습시킵니다.\n",
    "\n",
    "- [Data Collator](https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/data_collator)\n",
    "\n",
    "- [Trainer.TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) 의 group_by_length\n",
    "\n",
    "STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8348093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id, Unnamed: 0.\n",
      "***** Running training *****\n",
      "  Num examples = 149999\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 56250\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='804' max='56250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  804/56250 01:26 < 1:40:04, 9.23 it/s, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# 데이터 콜레이터 설정\n",
    "data_collator = DataCollatorWithPadding(tokenizer=huggingface_tokenizer)\n",
    "\n",
    "# 훈련 인수 설정\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    group_by_length=True,  # 길이에 따라 버킷화\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=hf_train_dataset,\n",
    "    eval_dataset=hf_val_dataset,\n",
    "    data_collator=data_collator,  # 데이터 콜레이터 추가\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
